{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpn7AekB6quw"
   },
   "source": [
    "____\n",
    "\n",
    "### <font color='magenta'>Plan: Week of Dec. 3rd</font>\n",
    "\n",
    "This is our last new material! \n",
    "\n",
    "Presentations are this Wednesday.  \n",
    "\n",
    "There is no more HW in the course, other than the extra credit HW for which you have until Friday at midnight; and, of course, you do not need to do any or all of it. Spend the time on your project only if you already have the grade you want in the course.\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyWiU0VJoTER"
   },
   "source": [
    "____\n",
    "\n",
    "# <font color='yellow'>Data Science in Sixty Four Dimensions</font>\n",
    "\n",
    "We will continue our brief study of classification with image data. \n",
    "\n",
    "Today we will remind ourselves of the basic EDA steps using a dataset type we haven't used yet in the class. The dataset is called \"MNIST\"; if you have played with ML in the past, you almost certainly know it, but don't worry if you have never heard of it before.\n",
    "\n",
    "‚úçÔ∏è Your first task is to read in the MNIST dataset. A simplified (smaller) version is already in `sklearn`, so let's grab that. It's called \"digits\". Read it in and look at what came in. In a markdown cell describe what `sklearn` gives to you. What form is the data in? Since we are doing classification, where does it store $X$ and $y$? What else does `sklearn` provide? What does $X$ look like (how it is provided to you)? For example, what is the shape of the data matrix $X$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1669315210525,
     "user": {
      "displayName": "Michael Murillo",
      "userId": "04445914509865448303"
     },
     "user_tz": 300
    },
    "id": "ILA_ykh2xbjJ",
    "outputId": "9b34572b-b563-44ce-a69f-8bb7addd00a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some code to get you started.....\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(f\"The data set has shape (rows, columns): {digits.data.shape}.\\n\")\n",
    "\n",
    "numbers = digits.data\n",
    "\n",
    "for data_point in range(3):\n",
    "  print(f\"Data point {data_point+1} is:\\n {numbers[data_point]}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVBEwvJgH3XG"
   },
   "source": [
    "Each data point provides $64$ numbers. This is a bit larger than, say, iris, which had $4$ numbers. In what you loaded, does `sklearn` provide feature names for the $64$ features?\n",
    "\n",
    "‚úçÔ∏è What are the feature names, and what are the classification targets? What are the target names?\n",
    "\n",
    "‚úçÔ∏è Discuss with your group the features: how are these features being used to allow us to handle an image? Does this strategy help use standard tools for this type of data, just like we used standard regression for handling some time series forecasting problems? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9a138pYKhZg"
   },
   "source": [
    "_____\n",
    "\n",
    "Ok, now that you are comfortable with what `sklearn` gave us, let's talk a little about this very famous dataset. \n",
    "\n",
    "The idea behind MNIST is to use ML to learn how to recognize handwritten numbers: the digits between $0$ and $9$. This is an interesting dataset because the data comes to us in the form of images, not just some numbers. While there are some pretty specific ways to handle images in ML, we'll use the simplest approach here. What we do is \"unravel\" the image into a string of features: each individual pixel of the image is a feature. Because the image has a lot of pixels, we end up with many features -- $64$ in this case. Iris and penguins only had $4$ features. To see this, let's put the image back to together and visualize the data. In `sklearn`, the images are square $8\\times 8$, so we can `reshape` them and make an image. \n",
    "\n",
    "Since you may not have done this before, I'll give you the code. \n",
    "\n",
    "‚úçÔ∏è Discuss with your group, and research on the internet, applications of what we are doing today. That is, list and discuss several ways of classifying images of all kinds. I'll get you started with one example: recognizing addresses in photos. \n",
    "\n",
    "‚úçÔ∏è Comment on the interesting parts of this code (below) to indicate that you know exactly what it is doing. Add titles to the images using the labels `sklearn` gave you. These labels are what we want to predict, given an image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "executionInfo": {
     "elapsed": 1706,
     "status": "ok",
     "timestamp": 1669312936176,
     "user": {
      "displayName": "Michael Murillo",
      "userId": "04445914509865448303"
     },
     "user_tz": 300
    },
    "id": "jgm50qvnxo2B",
    "outputId": "ffbc652b-8f4c-4cbc-fc60-575219431922",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_to_plot = 16\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for index in range(num_to_plot):\n",
    "  plt.subplot(1,num_to_plot,index+1)\n",
    "  plt.imshow(numbers[index].reshape((8,8)))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2MWfG5rNTBo"
   },
   "source": [
    "These digits may not look great to you; there are, in fact, [versions of this](https://en.wikipedia.org/wiki/MNIST_database) in $784$ dimensions to provide more resolution. I like this smaller $64$-dimensional dataset because when it works it is really impressive - it's harder for the ML. \n",
    "\n",
    "‚úçÔ∏è When you do this EDA/viz on the dataset, which digits do you think the ML will confuse? Later we will make a confusion matrix so that we can quantify what the classifers confuses; but, it is interesting for you to make your predictions now! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbszbCwOMDuy"
   },
   "source": [
    "‚úçÔ∏è Check the dataset for any problems you can see. Are there missing values? What are the numerical values? At this point, do you feel you need to scale the data? Why or why not? Are there other preprocessing steps needed at this point? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OxNCAdcVceQ"
   },
   "source": [
    "____\n",
    "\n",
    "\n",
    "‚úçÔ∏è Next, split the data into training and testing sets. Use `sklearn`'s `train_test_split`. [Read its docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and itemize what the various options are. When would you use these options? As you work through this, be sure to return back here to vary the options to see what they do. For example, for your project, you may find better performance if you don't use the defaults. \n",
    "\n",
    "‚úçÔ∏è This dataset has $>1700$ rows. How would you break this up into a smaller dataset with, say, $500$ rows? It'll be interesting to see what accuracy we can get as a function of the number of images we have to train with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGqGCTFTIYME",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agkEMH6HYQ3p"
   },
   "source": [
    "____\n",
    "## <font color='green'> Finally: Machine Learning Estimators </font>\n",
    "____\n",
    "\n",
    "We have our data understood, visualized, scaled and split: we are ready for the ML steps. Note that nothing we have done yet cares about the ML estimator we will use; in fact, we _could_ be doing regression at this point. \n",
    "\n",
    "I mentioned in the lecture last week that it is a good idea to use all of the classifiers at once. As we will see, this adds almost no complexity to your code, other than copying, pasting and changing a few words in each line. Importantly, you don't know in advance which estimator will work best on your data, so you should always **try all estimators**. Over time, you might build your intuition, or have some other reason for a particular choice, but for now use all/many of the classifiers. (Same is true for regression, clustering, etc.) \n",
    "\n",
    "I should mention a counter example that really illustrates the point: I wouldn't always use all of the `sklearn` algorithms for dimensionality reduction. Why? Because the algorithms do very different things: I would research the algorithms and pick the one that does what my problem really needs. The point is that if the estimators are all equivalent, you really have no reason to not use all of them; if you know they have a specialized function, then take that into account. \n",
    "\n",
    "Let's see what using many classifiers would look like. Lucky for you, people have already taken this philosophy and made very nice visualizations and comparisons. [Go to this website](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) and look at the figure. Look at the leftmost columns - there are three datasets (the rows) for classification in 2D. (We are working in $64$ dimensions, but not everyone can be us! üòÇ) Note that they compare $10$ different classifiers - these are the remaining columns. \n",
    "\n",
    "‚úçÔ∏è Open a markdown cell and give a summary of this visualization. This is a great way to learn the names of classifiers and see how they work. Which ones are really good or bad at a certain job? For example, look at the datasets:\n",
    "* the bottom one is perhaps easiest: blue on the left, red on the right; do they all get this right? \n",
    "* the top dataset makes this a bit harder by having the two classes fold into each in a way that doesn't follow a line - it's like two moons,\n",
    "* the middle dataset has one class inside the other; which fail for this case? \n",
    "\n",
    "In case you don't spot it right away: there are tiny scores given in the lower right of each image. Also, pay attention to whether the data points are solid or semi-tranparent: that's training versus testing - this gives you a sense for what the test-train split does for you. Include these insights in your discussion in the markdown cell. (You might want to increase the size of the plot on your screen!) \n",
    "\n",
    "You can quickly see that if you can't see your data, which we can't in $64$ dimensions, it would be very hard to know which estimator will work best. \n",
    "\n",
    "‚úçÔ∏è In another markdown cell, describe how they coded all of these estimators. That is, read through the code and see how they easily handled $10$ classifiers. How would you set up your code to do something similar? What you want to avoid is having $10$ separate codes, one for each estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taeFDDgGiNQr"
   },
   "source": [
    "____\n",
    "\n",
    "### Coding Classifiers\n",
    "\n",
    "It might be a good idea to put each of your group members in charge of one estimator. \n",
    "\n",
    "We are now ready for the ML part. We can break up the ML into four basic steps:\n",
    "1. `import` the estimator,\n",
    "2. create an instantiation of the estimator,\n",
    "3. train the estimator on the data\n",
    "4. make predictions! \n",
    "\n",
    "Of course, there are other steps you can and should add to this, but let's start simply. \n",
    "\n",
    "Typically, the first step will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q2Mi46VVLA9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPiGwjKLi1zv"
   },
   "source": [
    "Below, you will have many lines like this (or in a comma-separated list). This choice happens to be what is called a support vector machine (SVM), which we will assume for today is a black box estimator. (You'll learn the actual algorithm in a \"real\" ML course. SVMs are _very_ powerful.)\n",
    "\n",
    "The next step is to instantiate the estimator and assign it to an object. This looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq0N8JZIi_bK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(gamma=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6w0PhTijci-"
   },
   "source": [
    "There are three items of note here:\n",
    "1. note that we use the SVC library - because we are doing classification; SVMs can be used for regression as well (SVR),\n",
    "2. when you run the code, nothing happens! note that we are not yet using the data, \n",
    "3. there is a value of _gamma_ given; this is an example of a \"hyperparameter\". \n",
    "\n",
    "Next, let's use the estimator we just built and use it to fit to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DroijSytjiSC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_svm.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA95yXEZkJxL"
   },
   "source": [
    "Again, nothing obvious happens. What happened is that the SVM learned its parameters from the data for the case of a classifer with the value `gamma=0.001`. Now, the internal object knows this new information and we can use it to make predictions. \n",
    "\n",
    "So, final step: let's make predictions. At this point, you could deploy your machine in the real world and start using it to make predictions. But, the usual next step is to make predictions on the unseen test data we held back. This provides a check before we deploy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5Fku88XkFO9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6TXhlCUkmmE"
   },
   "source": [
    "Examine what this gives you. \n",
    "\n",
    "‚úçüèº In a markdown cell, describe what just happened and what is stored in the variable `predicted`. \n",
    "\n",
    "‚úçüèº Check accuracy for various estimators:\n",
    "1. plot the first few images in `X_test`, and put both the predicted and actual values in the title, as done [here](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py),\n",
    "2. modify the code above to have at least $5$ more classifers.\n",
    "\n",
    "‚úçüèº Visually, using the plot you just made, how well did the SVC work? Vary those things you can vary (e.g., rows in the dataset, hyperparameters) and explore how sensitive the predictions are to these choices. \n",
    "\n",
    "‚úçüèº Compute confusion matrices for several of the estimators and describe what they reveal. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
