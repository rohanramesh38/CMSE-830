{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mi-Fr8qlphRc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWeTaoSKscjx"
   },
   "source": [
    "![xkcd](https://imgs.xkcd.com/comics/machine_learning_2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVD6s7FFs7UE"
   },
   "source": [
    "____\n",
    "\n",
    "## Problem 1: Inverting a Matrix By Hand (10pts)\n",
    "\n",
    "I mentioned in class that we will mostly rely on Python libraries for performing linear algebra operations. Why not, isn't that why they were developed?!\n",
    "\n",
    "However, it is also good practice to know how to do these by yourself for the simplest cases. This allows you to explore ideas without a computer and build your intuition for what the libraries are doing. We'll learn here how to invert a $2\\times 2$ matrix by hand. Being able to do this by hand also gives you a test case to ensure you are using the Python libraries correctly.\n",
    "\n",
    "Follow these steps:\n",
    "1. Make a $2\\times 2$ matrix $A = \\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}$ using a NumPy array (you might want to try several choices); if you don't know about these already, NumPy has [some nice functionality for creating arrays](https://numpy.org/doc/stable/user/basics.creation.html), which can be matrices, of various types.\n",
    "2. Find the [determinant](https://en.wikipedia.org/wiki/Determinant) of your matrix, using:\n",
    "$$ \\mathrm{det}(A) = ad - cb.$$\n",
    "Do this by hand, not with a library.\n",
    "3. Form the inverse $A^{-1}$ with\n",
    "$$A^{-1} = \\frac{1}{\\mathrm{det}(A)}\\begin{bmatrix} d & -b \\\\ -c & a\\end{bmatrix}.$$\n",
    "Show all of your steps using $\\LaTeX$ in a markdown cell.\n",
    "4. Now that you have $A^{-1}$, use the rules of matrix multiplication to find the product $A^{-1}A$. Show your steps.\n",
    "5. Vary the matrix $A$ and comment on anything interesting you see. For example, what would $A$ look like if its determinant were $0$? For example, make a matrix for which $b=2a$ and $d = 2c$. What does this case correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GbDgAkw3I4g"
   },
   "source": [
    "____\n",
    "\n",
    "## Problem 2: Polynomial Regression (15pts)\n",
    "\n",
    "We have learned that linear regression refers to fitting data to a model in which the weights appear linearly. But, the model itself need not use linear functions. The Gaussian RBFs we used were an example of that. Another very common choice is that of a polynomial. Let's code two cases in which the number of weights is and is not the same as the number of data points.\n",
    "\n",
    "We wish to model our data with a polynomial. The data you are given is:\n",
    "$$x = [-2, -0.5, 0, 1] ,$$\n",
    "$$y = [0, 0.9375, 1, 3] .$$\n",
    "Because you have four data points, you are tempted to use a model with four parameters, such as:\n",
    "$$y = w_0 + w_1x + w_2x^2 + w_3x^3 .$$\n",
    "But, you are also worried that the data may be noisy so you **also** want to fit it to only three weights. You decide that the $x^3$ term could cause large excursions that might follow the noise, so your second model is:\n",
    "$$y = w_0 + w_1x + w_2x^2.$$\n",
    "\n",
    "Using only libraries from `linalg`, fit the data to both models. Plot the data and the two resulting models.\n",
    "\n",
    "As we have seen, the coding for this is trivial - the hard part is setting up the vectors and matrices. Slow down and be sure you understand what you are doing: this will help you set up the problem so that it is very easy. I'll give you a hint:\n",
    "$$\\underbrace{\\begin{bmatrix} y_1\\\\y_2\\\\y_3\\\\y_4\\end{bmatrix}}_{4\\times 1} = \\underbrace{\\begin{bmatrix} 0\\\\0.09375\\\\1\\\\3\\end{bmatrix}}_{4\\times 1} =\\underbrace{\\begin{bmatrix} 1 & -2 & 4 & 8 \\\\ 1 & -0.5 & 0.25 & 0.125 \\\\\\vdots  \\end{bmatrix}}_{4\\times 4}\\underbrace{\\begin{bmatrix} w_0\\\\w_1\\\\w_2\\\\w_3\\end{bmatrix}}_{4\\times 1}.$$\n",
    "It is crucial that you know where this came from!! Explain in a markdown cell where I got these numbers from.\n",
    "\n",
    "You then use Python to get the weight vector ${\\bf w}$, which allows you to plot the resulting polynomial. In one case you will need to use the pseudoinverse, [the `pinv` function](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html), or in [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.pinv.html#scipy.linalg.pinv), because you have more data points than weights.\n",
    "\n",
    "If you write your code in a general way, you can also try the third model:\n",
    "$$y = w_0 + w_1x.$$\n",
    "Do that next.\n",
    "\n",
    "In the world of machine learning, we would need to figure out which of these three models is the \"best\", a process called \"_model selection_\". We won't worry about it now, but I wanted you to at least be aware of the idea. A second idea I'll introduce here is \"_regularization_\", since it is connected. Regularization is penalizing weights that cause large excursions; here, we are doing this by hand by dropping the higher-order terms (e.g., setting $w_3=0$), thereby prohibiting predictions with large excursions.  There are very powerful techniques for automating this. There is a lot to learn from this problem other than just setting up regression problems and inverting a matrix! ðŸ¤“ In a markdown cell, comment on which model you think is best based on your plot(s) and explain your reasons. (If you take a machine learning course, you will learn powerful mathematical methods to make this judgement.)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vj2TdGXCFpK"
   },
   "source": [
    "____\n",
    "\n",
    "## Problem 3: Outer Product (5pts)\n",
    "\n",
    "Given the two vectors:\n",
    "$$v_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix} \\: v_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1\\end{bmatrix},$$\n",
    "compute the two outer products\n",
    "$$v_1 v_2^T,$$\n",
    "and\n",
    "$$v_2 v_1^T.$$\n",
    "Do this _both_ by hand, and show your work using $\\LaTeX$, and using a NumPy or SciPy library. Does the order of the vectors matter for the outer product? What about the inner product? Show and explain all of the details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17MBxKFqSAar"
   },
   "source": [
    "____\n",
    "\n",
    "## Problem 4: SVD (10pts)\n",
    "\n",
    "Read in the iris dataset (get it from anywhere you want) and perform SVD on it [using the `linalg` library](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html). Answer these questions:\n",
    "1. what does `svd` return? discuss and show with code\n",
    "2. in what form does it return Î£? the full matrix? `print` what is returned\n",
    "3. does `svd` return $V$ or $V^T$?\n",
    "4. what does a plot of the singular values $\\sigma_n$ versus $n$ look like? use both linear scale and log-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeCPWGS4c8ld"
   },
   "outputs": [],
   "source": [
    "# starter code\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "X = data[\"data\"]\n",
    "# print(X)\n",
    "\n",
    "# now, to SVD....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B8pv-MfSF-8"
   },
   "source": [
    "____\n",
    "\n",
    "## Problem 5: SVD Again! (10pts)\n",
    "\n",
    "We are going to use real (iris) data to understand the singular values a bit better. The idea is very simple: we want to take a data matrix and approximate it with a lower rank matrix, using the singular values as our guide.\n",
    "\n",
    "If you take the SVD and write it out in terms of the columns of $U$ and $V$, which are vectors, you will find that\n",
    "$$X = \\sum_{i=1}^r \\sigma_i {\\bf u}_i {\\bf v}^T_i .$$\n",
    "\n",
    "Be sure you understand the shape of these vectors/matrices! Are these matrix multiplications, inner products, outer products? Go slowly and be sure you understand what you are looking at. Explain in your own words what is in this expression. It might help if you derive it yourself.\n",
    "\n",
    "If we start in 2D and only use the first singular value in this sum, then the result should be...._in 1D_? Let's check!\n",
    "\n",
    "* Take the iris dataset and choose any two columns; in fact, try all combinations of two columns.\n",
    "* Plot one column versus the other.\n",
    "* From those two columns, find its SVD.\n",
    "* From the SVD for a new, smaller $X$ using only the $i=1$ term in that sum ($X \\approx \\sigma_1 {\\bf u}_1 {\\bf v}_1^T$).\n",
    "* Add to the plot you just made the data in the new matrix. (That is, plot one column versus the other, on top of the original data using the approximate $X$ from one SV.)\n",
    "* Explain what you learned from your plots.\n",
    "\n",
    "A hint is to use the result from the outer product library you explored above. See problem 3 above.\n",
    "\n",
    "There is one issue that could be confusing: what is ${\\bf v}^T_i$? This is the $i$th column of ${\\bf v}$, then transposed. It is **not** the $i$th column of $V^T$!! If it helps, [read this page](https://en.wikipedia.org/wiki/Singular_value_decomposition).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1668882157364,
     "user": {
      "displayName": "Vicente Amado Olivo",
      "userId": "06211665228838748236"
     },
     "user_tz": 300
    },
    "id": "9sEY8v_NVcTt",
    "outputId": "64b3d83e-13cf-4909-e426-980ac7f72979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Î£ that was returned is: [53.63004331  2.97631552]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# starter code\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "# choose two columns out of the four\n",
    "A = data[\"data\"][:,2:4]\n",
    "\n",
    "# let's look at it\n",
    "plt.plot(A[:,0], A[:,1], '^')\n",
    "plt.title(\"raw data\")\n",
    "\n",
    "# perform SVD\n",
    "U, S, VT = linalg.svd(A)\n",
    "V = VT.T\n",
    "\n",
    "print(\"The Î£ that was returned is:\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwsCmYecYCZt"
   },
   "outputs": [],
   "source": [
    "# helper code, if you need it...\n",
    "# if you use this, open a markdown cell and explain every detail\n",
    "# smaller = S[0]*np.outer(U[:,0],V[:,0])\n",
    "# plt.plot(smaller[:,0], smaller[:,1])\n",
    "# plt.plot(A[:,0], A[:,1], '^')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
